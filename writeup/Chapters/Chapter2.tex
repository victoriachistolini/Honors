% Chapter 1

\chapter{Approach} % Main chapter title

\label{Chapter2} 

%----------------------------------------------------------------------------------------


\subsection{Data used in Species Distribution Models}
% mention more detail about data, like predictors (ie raster images each pixel represents a value), each sighting entry has its lat/lon coordinates 
\noindent The database of tick encounters developed by MMCRI houses an enormous amount of information about the distribution of locations where ticks have been found in the state of Maine, however in its raw form, the data is missing a crucial metric: information about where ticks are not found. \newline

\noindent Classical modeling techniques use a set of predictor variables to classify events under certain conditions as likely or unlikely to happen. A logistic regression model, for example could take predictor variables about patients' heart rates and temperatures to determine the likelihood that the patient has the flu. In order to make a good prediction about the patient, the model needs heart rates and temperatures from patients who are healthy and from those who are sick in order to be able to differentiate sick from healthy metrics. In machine learning terminology, models like logistic regression are part of a class of models called supervised learning algorithms, because in order to discern patterns these models need examples of each category to be classified. \newline

\noindent Unfortunately, our tick dataset only provides information about locations where ticks are present and no information about the locations where ticks are absent, which motivates the need to use a different class of models. Unsupervised learning models are able to classify data that has not been labeled, by which group it belongs to, through using the patterns inherent in the data itself to distinguish and predict classification groups. However, in order to use unsupervised algorithms it is necessary that your data contain many examples from each category that you are trying to predict. Thus since our data contain only presence information and no absence data, we are unable build models for our data using unsupervised algorithms. \newline

\noindent Clearly when selecting an appropriate model for your data, it is essential to be certain that the type and quality of your data fits what is necessary and expected by the model building procedure to produce respectable results. If the model makes assumptions about your data, which do not hold, then the interpretability and usability of the model will be greatly impaired. In the literature on Species Distribution Modeling, the maximum entropy model is most commonly used, because the assumptions it makes about the data are well aligned with our purpose of modeling. Thus, we focus our research efforts on creating high preforming models using the maximum entropy model. \newline

%\subsection{Data Quality and Selection Bias} % is this good to have if so where????
%\noindent We need to address selection bias, spacial-auto correlation, correlation with roads, sampling effort is correlated with population density. \newline

%----------------------------------------------------------------------------------------
\section{The Maximum Entropy Model}

To determine the appropriateness of any modeling approach we first start by summarizing what information we know in the beginning. We know a sample of locations (latitude and longitude coordinates), where ticks have been found, called presence points. Assuming the presence points are well collected and representative of the locations and environmental conditions where ticks are likely to be found by humans, then we can use this information to estimate and predict locations with high likelihood of encounter. \newline

\noindent We have no information about unsuitable locations, where encounter risk is low, however, we can use our study region, the state of Maine as a constraint on our distribution. By taking a random sample of points from our study region, we create a representation of the diverse habitat conditions in our region, called background points. While we have a high degree of certainty that the presence points represent spaces where ticks should be found, we have no opinion at all about whether or not ticks should be found in the background locations. The theory of minimizing cross entropy ( which can be proved analogous to maximizing entropy)  says that given the information that we have, we can actually find a unique distribution that is optimized to the information that we know and does not penalize us for what we don't know, by selecting the distribution with minimal cross entropy [10].\newline


\noindent Put more technically, what we begin with are two probability density functions, $p(x)$ the distribution of tick from our presence and background points and $q(x)$, the probably density function of the tick encounter likelihood conditional on weather conditions and geographic constraints. The process of finding maximum entropy, then is  accomplished by minimizing the cross entropy function:  \newline

\begin{equation}
\int q(x)log(\frac{q(x)}{p(x)}) dx
\end{equation}


\noindent  During the minimization process, the predictor variables undergo transformations that help to maximize the entropy of the solution; this is how the model is fit [6]. The covariates may be transformed into terms of the type : linear, quadratic, product (representing interaction), hinge, threshold or categorical variables in the final model. The effectiveness of the transformations are assessed through cross validation process, using L1 regulation to avoid overfitting [6].  \newline


\subsection{Maximum Entropy Example}

\noindent When we began the modeling process, we had our dataset of tick observations, which highlighted the set of locations where ticks are known to be able to survive. We also start with another subset of point locations in the state of Maine, called the background points, which represent the landscape that we are trying to model. Perhaps the background points dataset consists of two environmental features, elevation and vegetation type and looks like this: 

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 x & Elevation & Vegetation Type & p(x) \\ [0.5ex] 
 \hline\hline
 1 & H &  1& 0.1  \\ 
 \hline
  2 & H & 3 & 0.1\\
 \hline
   3 & H & 2 & 0.1\\
 \hline
 4 & M & 2 & 0.1  \\
 \hline
  5 & L & 4 & 0.1 \\
 \hline
   6 & L & 3 & 0.1\\
 \hline
  7 & M & 5 & 0.1 \\
 \hline
   8 & M & 4 & 0.1\\
 \hline
 9 & L & 3 & 0.1 \\
 \hline
   10 & M & 3 & 0.1\\
 \hline
\end{tabular}
\end{center}


\noindent We know nothing more than this about the background locations, so we distribute probability to the subset of locations uniformly. That is if we were to go back to each of these locations then the chances of us getting a tick is equal in each location; so if we have $n$ observations in our set, then the probability of getting a tick at any of the locations would be $\frac{1}{n}$. This is a very naive estimate that each location is equiprobable, but this is our best first guess at the distribution and is called the prior distribution. By preforming a spacial analysis we may be able to find patterns between the locations of our tick sightings in the observations dataset, for example that 90\% of the locations were below a certain elevation. Based on this insight, which we call a constraint, we can recalculate the probabilities predicted for each location in our background point dataset. \newline

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 x & Elevation & Vegetation Type & p(x) \\ [0.5ex] 
 \hline\hline
 1 & H &  1& 1/30 \\ 
 \hline
  2 & H & 3 &  1/30\\
 \hline
   3 & H & 2 &  1/30\\
 \hline
 4 & M & 2 & 9/70  \\
 \hline
  5 & L & 4 & 9/70  \\
 \hline
   6 & L & 3 & 9/70 \\
 \hline
  7 & M & 5 & 9/70 \\
 \hline
   8 & M & 4 &  9/70 \\
 \hline
 9 & L & 3 & 9/70 \\
 \hline
   10 & M & 3 & 9/70\\
 \hline
\end{tabular}
\end{center}

\noindent Next we define another constraint on the model; we find that 82\% of the tick sightings are happening at vegetation types 4 and 5. We can incorporate this new constraint and reweigh the model. This time there is not as clear cut a way to reweight the probabilities. We know 18\% of the probability should be represented in the set $S_1$= \{ 1, 2, 3, 4, 6, 9,10\} and 80\% should be represented in the set $S_2$ = \{5,7,8 \} but how should the probabilities be determined for each element in the set? As mentioned previously, based on the principle of maximum entropy, we want to select the most uniform distribution by minimizing the entropy function in equation 1.2. This equation is difficult to solve analytically, thus numerical methods will be used for the optimization procedure. This toy example is based off of work done in [11]. \newline

\noindent In building the maxent model distribution, we have used constraints in order to add information to our model. There are several methods for figuring out how to derive the constraint rules. One of these processes is similar to the construction of a decision tree where at each new additional leaf we want to choose the feature that maximizes the information gain. We use the environmental features to develop the constraints which are the expected values of the features.\newline

\subsection{Current MaxEnt model}

\noindent The algorithm that we will be using to create our maximum entropy model is called MaxEnt and is supported by the R programming language through the dismo package; the main algorithm is written in Java, by S.J. Phillips et al (2006). We have currently developed several different MaxEnt models using environmental covariates that best fit the expert knowledge of the parameters that affect tick survival. The core set of environmental covariates came from North American Mesoscale Forecast System (NAM), in the form of raster images of a 12 km resolution. Each pixel of the image represents a projected latitude-longitude coordinate, and the value stored at the pixel is the value of the covariate at that location. For example if the raster image represented elevation, then the value at each pixel would be the meters above sea level at that location. \newline

\noindent The core parameter set includes: minimum air temperature, maximum air temperature, mean air temperature, mean percent vegetation cover, mean relative humidity, mean snow cover, mean snow depth, mean transpiration rate, mean u-direction wind speed (east-west winds), mean v-direction wind speed (north-south winds), mean wilt, sum of precipitation. Some models include additional parameters for previous-year's minimum winter temperatures.  \newline

\noindent Once a MaxEnt model is fit using the covariates mentioned above, the output model is of the form:
\begin{equation}
q(x) = \frac{e^{\lambda* f(x)}}{Z}
\end{equation}

\noindent where $\lambda $ is a set of weights on the features and Z is a scaling constant that makes sure the probability distribution $q(x)$ sums to 1 [12]. A new model is fit for each day of the year using all of the data in the database available for the particular day of the year. Since often there is not enough data for a single day of the year, data is taken from a window of time around the model day. The size for a suitable window depends on the time of year, with larger windows being needed during time of the year with fewer tick observations.\newline

\noindent Since we have created several MaxEnt models our goal is to perform exploratory analysis and look at potential sources of error and model refinement through the creation of new models with time-of-year specific parameter combinations to boost overall system performance. Since one main goal of our work is to develop a rigorous understanding of how covariates influence model performance, we need a statistically based framework to assess significance of parameter effect. The MaxEnt model does not allow us to preform hypothesis tests on coefficient significance for the output expression, thus we must use other methodology from spatial-temporal analysis to develop inference about covariates. 


%----------------------------------------------------------------------------------------
\section{Spacial-Temporal Exploratory Analysis} 

Exploratory data analysis is a crucial step in the model development process. It should occur before any model is attempted. It is here that the statistician is able to get to know the data. The key component of exploratory analysis is data visualization [14]. Exploratory analysis tells us something about the patterns generated from the data. It is also useful in identifying problematic elements of the data such as missing values or the nature of the scales at which the measurements are made. For example, does the scale of the raw data make sense, or should we transform the data, perhaps using a $log(x)$ transform or discretizing data which only takes on certain integer values. There is no standard procedure for exploratory analysis, however the major goal of the process is to  assess the data quality and to reveal data patterns. The same exploratory analysis tools are also used in examining the assumptions necessary for any parametric hypothesis tests.\newline

\noindent Aside from the broader questions mentioned above, our exploratory analysis seeks to also answer questions about the Spatial-Temporal aspects of our data. Spatial-Temporal analysis is the use of specialized statistical techniques that are designed to help identify patterns that derive from the spatial and temporal dimensions of the data [4].  In the sections below we address the questions we seek to answer through spatial-temporal analysis of the data, as well as the methodologies that we will use to accomplish these goals. \newline

\subsection{Spatial Analysis and Predictor Selection}
\noindent A branch of spatial analysis called point pattern analysis can help us understand how the tick observations are distributed spatially across different covariates and provide a statistical framework for hypothesis testing of these covariates. We will be using the poisson point process model for our experiments. A point process is a way of mechanistically thinking about point patterns and the processes that generate these patterns. These processes may be random, or they may be defined in terms of external factors [13]. \newline

\noindent One basic point process model is that generated by a completely random mechanism, which we will call a point process with complete spatial randomness (CSR). CSR point processes have two special properties, (1) they are homogeneous, meaning that the intensity of points does not depend on the location within the experiment window, and (2) the points are independent of one another; meaning that the location of one point cannot influence the placement of a nearby point [13]. An inhomogeneous point process (IHP), is a point process such that the intensity of points depends on spatial location [13]. We will want to develop a test that can determine if our tick data follow a CSR process (the null hypothesis) or an IHP pattern ( the alternative hypothesis). \newline 

\noindent In order to develop the statistical test mentioned above, we need to make some observations about point patterns. If we assume that the study space of the points is well defined and that an observation could have occurred anywhere in the study space, and that the set of observations is a complete enumeration of all observations, then it can be shown that the points follow a poisson distribution. Informally, this can be seen by imagining the study space divided into infinitesimally small regions, such that most regions have no points, then by the law of the frequency of rare events, the points follow a poisson distribution [13].  \newline

\noindent Since we suspect that environmental factors are the major mechanisms driving the human-tick observation pattern, then the human-tick observation pattern is an IHP type pattern. We want to test this hypothesis  that human-tick observations follow an IHP, by controlling for the covariate process and then testing for a CSR process. Figure 1.1 provides an graphic explanation of this process: in A the IHP from day 150 and years 2006-2013 clearly is clustered in space. It is important to note that the right boundary, added in blue in B is the Atlantic Ocean and is actually not part of the study region for the purpose of calculations. In B the boundaries of a fictitious covariate are added, one could think of these boundaries as temperature gradient lines. The point patterns in each section appear to be more in line with a CSR process in that the points appear to be randomly distributed and independent of one another. Although this example has been fabricated it is a good illustration of what we seek to do. However, controlling for a single covariate may not be enough to achieve CSR, one might need to add several terms, including interaction and indicator terms as well.\newline
 
 
\noindent The fitted poison point process model is of the form:

\begin{equation}
\lambda(x) = e^{\alpha +\beta(x)}
\end{equation}

\noindent We can conduct hypothesis tests on each of the $\beta_i$ coefficients to discern the effectiveness of the parameters in the model. \newline


\begin{figure} [t]
\centerline{\psfig{figure=Figures/CSR_exp.pdf,width=4in,height=3in}}
\caption{An example of Complete Spatial Randomness derived by controlling for covariates. }
\label{fig6}
\end{figure}

\noindent We must be careful about some of the assumptions that we made, and how they relate to our interpretation of the model. The assumption of having the points be a complete enumeration is clearly not met. This causes the model to penalize all areas that do not have an observation where a tick incident occurs (due to the event not be recorded). Although this assumption might be too restrictive if we were trying to create a mapping of all of the possible locations for ticks, this is not our goal, we are trying to map human-tick encounter risk. Thus we can argue in favor of penalizing the locations with few to no observations if they have lower populations and thus little chance of an encounter there in the first place\newline


\subsection{Temporal Analysis}
% do predictor sets change over time
Ideally we are trying to create a forecasting model for each day of the year. The need for models that are season or even month-day specific is based off of the fact that the weather conditions can vary greatly throughout the year, as with the behavioral patterns of the humans who are going to encounter ticks. Therefore, when designing our experiments of predictor selection, it is important to repeat the experiments to capture the yearly variability. One way to do this is to run a predictor selection experiment for a day mid-month for each month in the year. \newline

% window experiments
\noindent A temporal limitation to the analysis is the limited number of observations at different times of the year. Ideally we will be looking to make a model for each day of the year to use for forecast prediction. However, due to the fact that one is less likely to encounter a tick during the winter months, we have fewer observations for this time frame. With fewer observations to build the model, the forecast predictions are less accurate. \newline 

\noindent Thus, we must preform an experiment to understand the impact the limited amount of data has on the model?s subsequent accuracy. We seek to understand if aggregating data from months with few observations will positively impact accuracy. We also seek to determine a threshold about the number of observations necessary to achieve a certain error tolerance and a bound on the window size based on day of the year. \newline

% stationarity of the data set
\noindent Another area of interest is the stationarity of the data. There is evidence that the range of suitable tick habitats continues to expand as the effects of global climate change become more dramatic [5]. At this point in our primary analysis we have not explored stationarity in our dataset. We expect that there will be some correlation with the tick distribution, its predictors and time. In order to test this correlation, we can build regression models for each predictor at each location across time to quantify the degree and strength of non-stationarity in the data.  \newline


%----------------------------------------------------------------------------------------


\section{Model Assessment }  % FIX include stuff about FAUC 

\subsection{Assessment of Performance }

% draw a confusion matrix here....
Assessment of classification algorithms is an essential part of determining their utility as valid predictive models. Classically testing the strength of a classification algorithm involves building the model with a subset of the dataset and keeping another subset for use in testing the model called the training dataset. Then the model is run with the training data and an assessment is made about the model performance. The construction of a confusion matrix helps to communicate how well the model did in classifying the training data by tabulating the number of correctly classified results as well as false positives and false negatives. 

\begin{equation} \bordermatrix{~ & Observed \hspace{0.2cm} Presence & Observed \hspace{0.2cm} Absence \cr
                  Predicted\hspace{0.2cm}  Presence & a & b \cr
                  Predicted \hspace{0.2cm} Absence & c & d \cr} \end{equation}
                  
\noindent The data in the confusion matrix can then be visualized in a plot called the ROC curve. \newline

\subsection{The AUC statistic for model assessment }

The ROC curve plots the classifier's sensitivity versus 1-specificity, where sensitivity is easily calculated from the confusion matrix as  $\frac{a}{a+c}$ and 1- specificity is $\frac{b}{b+d}$ [7]. Since the results of the MaxEnt model are a series of probabilities given environmental conditions of encountering a tick at a particular location, then in order to create a confusion matrix, we would need to decide on some arbitrary threshold at which we decide that a probability is high enough to be considered a presence of a tick. \newline

\noindent Since the decision of a threshold is arbitrary, then an ROC is formed from finding the confusion matrix for each threshold, which will give a new sensitivity and 1-specificity value to plot for each matrix. The summary statistic used to characterize the ROC curve is the area under the ROC curve or AUC, which evaluates the strength of the classifier by the characteristics of the ROC curve. An AUC statistic of 0.5 represents a random classifier, and scores above 0.5 represent a better than random model [7].    \newline

\noindent Since we are using presence only data, the traditional methods of model assessing the ROC curve fall short. The training dataset is a series of presence observations, thus there would be 0 observations correctly classified for no encounter, cell d,  which would make it impossible to calculate 1- specificity. Since we can't calculate 1-specificity, we can proxy it using the proportion of area predicted present [7] [MORE DETAILS].  With the simple modification under way we can proceed to interpret the AUC in a similar fashion as our interpretations of the traditionally defined AUC.  \newline


\subsection{The F-AUC statistic for assessment of future predictive power }

The F-AUC or forecast AUC, is another validation technique used to assess the strength and performance of models. A model is built with a given predictor set, using only a subset of the data in the database, a training dataset, then performance is assessed using the rest of the data, the testing dataset. For example years 2006-2010 of data are used to build the model, then the model is tested using data from 2011-2013, using the 2006-2010 model to predict the outcomes for the 2011-2013 data. Partitioning our data into testing and training sets allows us to more accuratly estimate of the true predictive power of the model. 

\subsection{Limitations of AUC Interpretability}
\noindent One limitation of the AUC is that it represents the classification ability of the model independent of a threshold. The threshold determines the minimum probability that will be considered a presence. In our application this threshold is important because we are creating a forecast and the interpretability of a forecast greatly depends on the ability to discriminate events from non-events, thus the there seems to be an implied necessity to define the threshold as a single number when calculating a summary statistic about the skill of the forecast. The statistic True Skill Statistic (TSS):
\begin{equation}
TSS \hspace{0.2cm} = \hspace{0.2cm} sensitivity \hspace{0.2cm} + \hspace{0.2cm} specificity\hspace{0.2cm}  -\hspace{0.2cm} 1 \hspace{0.2cm} =  \hspace{0.2cm} \frac{ad -bc}{(a+c)(b+d)}
\end{equation}
can be used used to quantify the strength of the classifier at a given threshold [9]. The TSS statistic has been shown to have a good behavior and is well correlated with the AUC statistic [9]. However, in order to calculate the TSS statistic we would need to have access to a complete confusion matrix which we do not have. Therefore, in order to use the TSS, we would need to substitute our proxy metric calculated at a given threshold. \newline



%----------------------------------------------------------------------------------------

\section{Ensemble Models}

Since we are trying to develop models to forecast human-tick encounter for the entire year, it is reasonable to assume that it is unlikely that a single model will be able provide the best predictions for every day of the year. However, it is reasonable to believe that a collection models with different parameter combinations could be specialized to preform better at different times of the year. Evidence has shown that collections of models, called ensemble models, provide more robust forecast models, where each of the individual models in the ensemble provides independent and novel information to contribute to a collective consensus [15]. \newline

\noindent An ensemble of models is defined as creating duplicated models with altered initial conditions, boundary conditions, types of model, and parameter combinations [15]. When we create a model there are many sources of uncertainty, we do not know the true mechanisms that drive human-tick encounters. As a result, we must take educated guesses at which predictors are useful to include. We began by intuitively selecting core parameters and then use the statistical framework of the poisson point process to guide predictor selection for different times of the year.  We also performed exploratory experiments on the window size parameter at different times of the year, perturbing it based on season and subsequent observation density.  \newline

\noindent Despite having created many new models, looking specifically at conditions at certain times of the year to fit the highly variable observation patterns, it is possible that our ensemble is underdispersive. An underdispersive ensemble means that the ensemble is not as variable as would be expected given the diversity of its members [16]. \newline

\noindent Variability is desirable in an ensemble because, it ensures robustness and representation of future potential variability [15] Underdispersion occurs because it is very difficult to capture all of the types of uncertainty that exist. Therefore, is a trade off between exhaustively searching the multi-dimensional space of model uncertainty versus spending effort in creating an ensemble of fewer, perhaps less variable, but individually more skillful members. There is evidence that focusing energy on improving the quality of individual models produces higher quality results. [15]


\subsection{Future Work with Ensembles}
 
Currently we have done the first step of creating an ensemble by selecting a set of skillful models that have strengths at different times of the year. Currently we do not have enough information to distinguish which models preform better than others conditional on the time of the year. Future work includes more rigorous testing of the ensemble members on future collected data and storing model performance over the long term. With this information we would be able to discern which models are more skill full an properly calibrate the ensemble as described in [16]. 


%----------------------------------------------------------------------------------------
