% Chapter 2

\chapter{Exploratory Analysis} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
%\newcommand{\keyword}[1]{\textbf{#1}}
%\newcommand{\tabhead}[1]{\textbf{#1}}
%\newcommand{\code}[1]{\texttt{#1}}
%\newcommand{\file}[1]{\texttt{\bfseries#1}}
%\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Looking at distributions }

We start by looking at how the raw data of our observations set is distributed. We are looking to create a testing and training dataset that are distributed similarly. Figure 2.1 shows two plots of the number of observations recorded on a given day of the year for the time period 2006-2010 (A) and 2011-2013 (B). Although in (A), we see that there are overall more points per day, the yearly shape of observation records is strikingly similar between these two sets. The first and third quarter of the year have few observations per day, while the second and forth quarters of the year have increasing activity, which spikes mid quarter and then declines for the second half of the quarter. 

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/obs_dist.pdf,width=4in,height=3in}}
\caption{Distribution of tick observations during the year for (A) years 2006-2010 and (B) year 2011-2013. }
\label{fig6}
\end{figure}


\noindent Given the yearly activity cycle of the tick, we need to be cognizant of the number of points being used to create each model. Clearly, there will be a much smaller pool of observations during the first and third quarters. Many days may have zero observations even as we are aggregating data from a span of almost 10 years. On days with so few data points it becomes impossible to create a model. One way around this obstacle of low observations counts is to pull in observations from a window of time around the forecast date. We will call this new parameter the window size of the model. \newline

\section{Window size implications }

\noindent The window size parameter has a lot of uncertainty around it. It is unclear what values this parameter should take on at different times of the year. Further we do not know the impact of increasing the window size parameter on the accuracy and precision of the model. One hypothesis is that it is necessary to keep the window size parameter large enough so that the model has exposure to enough data to create well-informed predictions. Another hypothesis is that if the window size is increased by too much, then the accuracy of the predictions will be weakened due to the presence of data irrelevant to the current stage of tick activity. \newline

\noindent In order to assess the impact of window size on forecast skill, we run a MaxEnt model with a subset of the 13 core parameters as predictors against 7 candidate window sizes: $\pm$ 2, 3, 7, 15, 20, 30, 40 days, for 13 days of the year (approximately the 15th day of each month is tested); days 15, 46, 76, 105, 135, 146, 166, 196, 227, 258, 288, 319, 349 of the year. \newline

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/wsize.png,width=4in,height=3in}}
\caption{AUC score by day of year for the 2006-2013 time period of observations. Each line represents a different window size from the 7 candidate sizes. }
\label{fig6}
\end{figure}

\noindent The general trend of figure 2.2 independent of window size is the inverse of figure 2.1. Highest AUC's are seen during the first and third quarters of the year, while steep crashes in AUC scores are experienced during the second and forth quarters. It is important to note that as a rule, the fewer points we have in our model, the higher the AUC will be because there is a lower chance of being wrong. For example, the null model of simply guessing that no location has high probability of tick encounter means that with fewer points this model will be more correct than the exact same model judged with more points. However, it is clear that the null model is a very poor choice of a model if future forecast skill is of concern. \newline

\noindent Thus in order to correctly interpret figure 2.2, we focus on the first and third quarters of the year. For the first 25\% of the year it seems that the accuracy is about equal for all window sizes, and thus it is likely useful to include a larger window size so that the model has access to more information at fit time. By around day 70 though, this is not entirely true, windows that are too large such as the sizes 30 and 40 produce much weaker AUC scores. The window size of 15, however, continue to perform well high, so it would be advisable to use window sizes up to 15 during this time of the year. Even maintaining a window size as large as 7 into the beginning and at the end of second quarter seems justified based on continued high performance of the 7 day window during these transitional times. For the third quarter a window of size 15 seems to have the best balance of performance and inclusion of data. For the second and fourth quarters, windows sizes or 1 - 2 days are sufficient for high performance because there is plenty of data amassed even for small windows during this time. By the end of the fourth quarter, however, as winter begins to take hold, window sizes of 15 or 20 are best. \newline

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/num_obs.pdf,width=3in,height=4in}}
\caption{AUC score by number of observations for the 2006-2013 time period of observations. Each dot comes from one of the 13 days of the year mentioned above, with one of the 7 window sizes. }
\label{fig6}
\end{figure}

\noindent Another metric that we can use to assess the window size parameter is by the number of points contained within a given window size. Figure 2.3 shows a decreasing trend of AUC values produced by having too large of a window size. Up to around 200 points the models seem to produce high quality results with AUC scores hovering around 0.9. However, once we go past 500 points, AUC scores start to tank, indicating that window sizes generating such high point values are probably too big to provide valuable information.  \newline



\section{Understanding predictor variables}

Before we try to fit models to our observation set, it is necessary to understand what the distributions of our predictor variables look like. This will allow us to identify is there might be some kinds of transformations to the covariates that might be beneficial to our model. In Figure 2.4, we have created histograms of all of the predictor variables for day 150 of 2007. Day 150 is around the height of the first peak in tick activity. We can see that variables like mean air temperature and wind have distributions that appear continuous in nature. However, transpiration rate, wilt and vegetation cover appear to have discrete distributions of values. 

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/2007_day_150.jpg,width=4in,height=4in}}
\caption{Histograms of the distributions of core predictor  variables for day 150 of 2007. }
\label{fig6}
\end{figure}

\noindent The vegetation cover variable is categorical, with 20 different categories, based on the International Geosphere-Biosphere Programme (IGBP) land cover classification system. According to figure 2.4, the most popular vegetation type for tick observations is 11, permanent wetlands, as well as 12-15 which represent cropland and mix vegetation types of forest, shrub and grassland. A more through examination about how vegetation cover of where ticks are found changes throughout the year, reveled another important vegetation type in the second and fourth quarters of the year to be 5, mixed forests. Since a categorical variable with 20 categories does not seem reasonable since so few categories are actually represented, we create a binary indicator variable, called v4 that represents if a tick is found in vegetation type 11-15 or not. 


\section{Poisson point process models for predictor selection and evaluation}

Using the insights that we gained from the exploratory analysis of our dataset we begin with the first step of the ensemble creation process. In section 1.5, we outlined the process for creating an ensemble which involves perturbing initial conditions (in our case window size and study space boundaries), using different model types or creating novel parameter combinations. We will focus on creating novel parameter combinations that are specialized to certain times of the year and using the statistical frame work of the poisson point process model to assess the significance of each of the parameters in the new models. \newline

\noindent We build poisson point process models for 13 days of the year (approximately the 15th day of each month is tested); days 15, 46, 76, 105, 135, 146, 166, 196, 227, 258, 288, 319, and 349 of the year. We derive the window size parameter by trying to minimize the window size while maintaining a good coverage of observation data over the study space. If increasing window size does not seem to add more coverage and 50 or more points are included, then we do not expand the window size. These judgements are made qualitatively by looking at point spread across the map of Maine. On the other hand sometimes we increase the window size as much as up to $\pm$ 20 days, a size which evidence from the previous exploratory window experiments reveals is about as large as possible without sacrificing too much performance over the long term, and we still only have a few points. Despite having little information, we build a model with what we have.  \newline

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/pseudo_code.png,width=4in,height=2in}}
\caption{Pseudo code for the predictor selection algorithm. }
\label{fig6}
\end{figure}
\subsection{ Predictor reduction algorithm}

For each of the 13 days of the year mentioned above, download the tick observation data from the time span of 2006-2013, with the window size chosen by hand using the method described above. Data about the window sizes selected for each of the experiments run is shown in table 2.1. We also extract data for each of the core 13 covariates from the raster images at the pixels where each observation in the day and window range being studied occurred. \newline

\noindent We begin parameter selection by generating a correlation matrix of all of the 13 covariates. Trying to create a poisson point process model with covariates that are highly correlated impairs the performance of the model. Once we have information about the highly correlated predictors we are able to create subsets of the parameters that do not contain any of the highly correlated predictors. Then we build a poisson point process model using the command ppm() from the spatstat package in r. Once we have a model built, we prune unnecessary predictors using a greedy, stepwise algorithm, which removes the predictor with the lowest z-score (an indication of its statistical significance) until all predictors are significant at the 5\% level.  \newline

\subsection{ Results from predictor selection experiments}

Figure 2.6 shows the summarized results of the predictor selection experiments, while table 2.1 shows detailed descriptions of the number of observations and number of points used for each experiment. In figure 2.6, highlights many striking patterns throughout the year. We see that during the winter and early spring, the predictors mean air temperature and v-direction wind appear consistently up to day 150 in the year. \newline

\noindent Based on figure 2.1, day 150 occurs approximately at the first peak in tick activity, which coincides with the onset of late spring / early summer. The predictors of highest importance during this time period are the relative humidity, the temperature range (minimum air temperature and maximum air temperature), the type of vegetation and the amount of precipitation. Late summer and fall show a renewed importance of mean air temperature, u-direction wind and vegetation type.  \newline

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/summary.png,width=4in,height=4in}}
\caption{Histograms of the distributions of core predictor  variables for day 150 of 2007. }
\label{fig6}
\end{figure}


 \begin{longtable}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \caption{Parameter combinations  and meta data of ensemble members\label{long}}\\

 \hline
 \multicolumn{4}{| c |}{Selected Models}\\
 \hline
 Day of Year & Window Size & Number of Observations & Predictors\\
 \hline
 19   & 20    &12 &   meanAirtemp, vwind, sndepth, sumPrecip \\
 \hline
 19 &   20  & 12   & meanAirtemp, v4 \\
 \hline
 50 & 20 & 11 &  meanAirtemp,  vwind, sncvr\\
 \hline
 50 & 20 & 11 &  meanAirtemp,  vwind, v4\\
 \hline
 78  & 7 & 117 &  meanAirtemp, meanVegcvr, vwind, sumPrecip, sncvr, v4\\
 \hline
 109 &  3   & 118 & meanAirtemp, uwind, meanVegcvr, vwind, minAirtemp, sumPrecip, v4 \\
 \hline
 139 & 3  & 233  & meanAirtemp, uwind, meanVegcvr, vwind, wilt, sumPrecip \\
  \hline
 139 & 3  & 233  & meanAirtemp, meanVegcvr, vwind, sncvr \\
  \hline
 139 & 3  & 233  & meanAirtemp, uwind, meanVegcvr, vwind, wilt, sumPrecip, v4 \\
 \hline
 139 & 3  & 233  & meanAirtemp, meanVegcvr, vwind, sndepth, v4 \\
  \hline
 139 & 3  & 233  & meanAirtemp, meanVegcvr, vwind, sndepth \\
 \hline
 150 & 7  & 445 & meanAirtemp, meanHumidity, meanVegcvr, uwind, vwind, sumPrecip\\
 \hline

 \endfirsthead
 \hline
 \multicolumn{4}{| c |}{Selected Models Continued }\\
 \hline
  Day of Year & Window Size & Number of Observations & Predictors \\
  \hline
  \endhead


 \hline
 150 & 7  & 445 & minAirtemp, meanVegcvr, uwind, vwind, sumPrecip, v4\\
 \hline
150 & 7  & 445 & meanAirtemp, meanHumidity, uwind, sumPrecip, v4\\
 \hline
 150 & 7  & 445 & minAirtemp, meanVegcvr, uwind, vwind, sumPrecip, v4\\
  \hline
 170 & 7  & 262 & meanAirtemp, meanHumidity, uwind, vwind, sumPrecip, v4\\
 \hline
 170 & 7  & 262 & minAirtemp, meanHumidity, uwind, sumPrecip, v4\\
 \hline
 170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, vwind, sumPrecip, v4\\
 \hline
  170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, trnstr, sumPrecip, v4\\
 \hline
   170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, trnstr, sumPrecip, v4\\
 \hline
   170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, vwind, wilt, sumPrecip, v4\\
 \hline
    200 & 10  & 61 & wilt, sumPrecip, v4\\
 \hline
     231 & 13  & 13 & meanAirtemp\\
 \hline
      231 & 13  & 13 & trnstr\\
 \hline
      231 & 13  & 13 & wilt\\
 \hline
       262 & 10  & 62 & meanAirtemp, uwind, vwind, sumPrecip\\
 \hline
        292 & 3  & 553 & meanAirtemp, uwind, meanVegcvr, v4\\
 \hline
         292 & 3  & 553 & maxAirtemp, uwind, wilt, v4\\
 \hline
          292 & 3  & 553 & maxAirtemp, uwind, trnstr, v4)\\
 \hline
          323 & 3  & 250 & meanAirtemp, trnstr, uwind, vwind, v4)\\
 \hline
           323 & 3  & 250 & meanAirtemp, wilt, uwind, vwind, v4)\\
 \hline
            323 & 3  & 250 & meanAirtemp, meanVegcvr, uwind, vwind, v4)\\
 \hline
             353 & 10  & 34 & meanHumidity, uwind, meanVegcvr, v4)\\
 \hline
\end{longtable}

\section{Creating MaxEnt models}

Having determined combinations of predictor variables that are specialized for different times of the year, we can now build maximum entropy models with these new predictor combinations. We create a new maxent model for each of the 13 studied days of the year, using the window sizes listed in table 2.1, for each of the 32 predictor sets, resulting in 416 maxent models. We calculate the AUC statistic of each model to indicate how well the models fit the data that they were trained on. \newline

\noindent Looking back at figure 2.1 we see the two plots of the number of tick observations on given days of the year. We see that the year trace in figure 2.1 (A)  from 2006-2010 and 2011-2013 in (B) have the same trajectory and thus represent a good split of the data. We use the time span from 2006-2010 to build the 416 maxent models and will use the time span 2011-2013 in section 2.6 to evaluate model performance on testing data that it has never seen before. \newline

\subsection{ MaxEnt model fit results}

One way to easily assess model performance is to evaluate how well the model fits the data that it was trained on. We can think of this step in model evaluation like interpreting a correlation coefficient. The correlation coefficient tells us how well a linear model fits the trends in the data, however it does not indicate anything about how well the model will preform in the task of accurately classifying future data. The AUC statistic provides the same function for MaxEnt models, acting as an indication of goodness of fit. \newline

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/model_aucs.png,width=6in,height=4in}}
\caption{AUC results from running the 32 different covariate combinations. }
\label{fig6}
\end{figure}


\noindent Figure 2.7 shows the AUC 32 predictor combinations MaxEnt models fitted at each of the 13 time points in the year. In the winter months, there is quite a large degree of variability between the models, which tightens quite a bit during the early spring. As we approach mid-way through the second quarter mode fit tanks and there is enlarged variability again. In the third quarter, the models regain some of their quality performance around mid summer, which remains stable although highly varied until fall what the variability decreases and peaks quickly, followed by a quick crash and slow regain of fit into the new year. 


\section{Evaluating MaxEnt models}

Now that we have built and assessed the goodness of fit of all of our 416 MaxEnt models, now we want to utilize the testing dataset that we saved from years 2011-2013 to evaluate the model's performance on data that is has not yet seen. Figures 2.8 and 2.9 show the F-AUC scores of each of the models at the 13 time points of the experiment. The models in figure 2.8 so poor performance up through the first half of the first quarter at around day 60. From a peak at day 60, the models begin a gradual decline in performance up through the beginning of the fourth quarter. During this time period the variability between models is rather low. During the fourth quarter we see increased performance and greater between model variability. \newline

\noindent Although it intuitively make seem that it is good for the ensemble to have low variability, since it may seem like the models are providing a consistent answer, this is not the case, it is ok to have variability between models and in fact this is a desired quality of an ensembles. When ensemble have too little variability between models it is a sign of ucnderdispersion and indicates that the ensemble needs calibration [16]. So it is a good sign that we see variability in our ensemble and not one model dominating the other models, a contrast to what we saw in figure 2.7. \newline


\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/group1_Fauc.png,width=6in,height=4in}}
\caption{F-AUC results from a subset of the 32 predictor combinations that have high performance year round. }
\label{fig6}
\end{figure}

\noindent Figure 2.9 shows the second group of models from the 32 original sets. This group is characterized by two steep crashes in performance in the third and fourth quarters of the year. At the beginning of the year, these models do much better than those in figure 2.8, along with their high performance there is a healthy variability and no single model is dominating the field. High volatility is seen in the third and beginning of the fourth quarter, and it is unclear what is causing this, since the steep descents do not seem to line up with periods of highest activity unlike in figure 2.7.  The models recover performance by the end of the fourth quarter. \newline


\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/group2_Fauc.png,width=6in,height=4in}}
\caption{F-AUC results from a subset of the 32 predictor combinations that have two steep dips in performance throughout the round. }
\label{fig6}
\end{figure}

\noindent  If we look at the patterns of the covariate combinations that make up the models in figure 2.8 and figure 2.9, we can see some interesting patterns. In Figure 2.10, we can see that the models in figure 2.8, who have a fairly consistent trend year long, have a high emphasis on the vegetation cover and vegetation type parameters as well as both u and v direction wind and mean air temperature. \newline

\noindent However, we see in figure 2.11, that the models that have more yearly volatility emphasize parameters such as the range of air temperature ( minimum and maximum air temperature) as well as sum of precipitation, u-direction wind, but most surprisingly is the almost complete absence of the mean vegetation cover predictor. \newline

\begin{figure} [t]
\centerline{\psfig{figure=Figures/group1.png,width=4in,height=2in}}
\caption{Summary of predictor combinations from figure 2.8. }
\label{fig6}
\end{figure}

\begin{figure} [t]
\centerline{\psfig{figure=Figures/group2.png,width=4in,height=2in}}
\caption{Summary of predictor combinations from figure 2.9. }
\label{fig6}
\end{figure}

  
\section{Future directions}

Now that we have identified a set of candidate models and have evidence that these models provide a decent amount of variability and quality performance, it is necessary to collect more data about their performance over the log term. By keeping track of how well the models do on future data, we will then be able to build up a better understanding of long term performance of the models and different points in the year so that we can come up with a posterior weighted averaging of the models based on the knowledge that we have gained. This will help calibrate the ensemble and create a further robust and effective system. \newline

