% Chapter 2

\chapter{Exploratory Analysis} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
%\newcommand{\keyword}[1]{\textbf{#1}}
%\newcommand{\tabhead}[1]{\textbf{#1}}
%\newcommand{\code}[1]{\texttt{#1}}
%\newcommand{\file}[1]{\texttt{\bfseries#1}}
%\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Looking at distributions }

We start by looking at how the raw data of our observations set is distributed. We are looking to create a testing and training dataset that are distributed similarly. Figure 2.1 shows two plots of the number of observations recorded on a given day of the year for the time period 2006-2010 (A) and 2011-2013 (B). Although in (A), we see that there are overall more points per day, the yearly shape of observation records is strikingly similar between these two sets. The first and third quarter of the year have few observations per day, while the second and forth quarters of the year have increasing activity, which spikes mid quarter and then declines for the second half of the quarter. 

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/obs_dist.pdf,width=4in,height=3in}}
\caption{Distribution of tick observations during the year for (A) years 2006-2010 and (B) year 2011-2013. }
\label{fig6}
\end{figure}


\noindent Given the nature of this yearly activity cycle of the tick, we need to be cognizant of the number of points being used to create each model. Clearly, there will be a much smaller pool of observations during the first and third quarters, however, many days may have zero observations even as we are aggregating data from a span of almost 10 years. On days with so few data points it becomes impossible to create a model. One way around this obstacle of low observations counts is to pull in observations from a window of time around the forecast date. We will call this new parameter, the window size of the model. \newline

\section{Window size implications }

\noindent The window size parameter has a lot of uncertainty around it. It is unclear what values this parameter should take on at different times of the year. Further we do not know the impact that increasing the size of the window parameter has on the accuracy and precision of the model. One hypothesis is that it is necessary to keep the window size parameter large enough so that the model has exposure to enough data to create well informed predictions. Another hypothesis is that if the window size is increased by too much, then the accuracy of the predictions will be weakened due to the presence of data irrelevant to the current stage of tick activity. \newline

\noindent In order to assess the impact of window size on forecast skill, we run a MaxEnt model with a subset of the 13 core parameters as predictors against 7 candidate window sizes: $\pm$ 2, 3, 7, 15, 20, 30, 40 days, for 13 days of the year (approximately the 15th day of each month is tested); days 15, 46, 76, 105, 135, 146, 166, 196, 227, 258, 288, 319, 349 of the year. \newline

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/wsize.png,width=4in,height=3in}}
\caption{AUC score by day of year for the 2006-2013 time period of observations. Each line represents a different window size from the 7 candidate sizes. }
\label{fig6}
\end{figure}

\noindent The general trend of figure 2.2 independent of window size is the inverse of figure 2.1. Highest AUC's are seen during the first and third quarters of the year, while steep crashes in AUC scores are experienced during the second and forth quarters. It is important to note that as a rule, the fewer points we have in our model, the higher the AUC will be because there is a lower chance of being wrong. For example, the null model of simply guessing that no location has high probability of tick encounter means that with fewer points this model will be more correct than the exact same model judged with more points. However, it is clear that the null model is a very poor choice of a model if future forecast skill is of concern. \newline

\noindent Thus in order to correctly interpret figure 2.2, we focus on the first and third quarters of the year. For the first 25\% of the year it seems that the accuracy is about equal for all window sizes, and thus it is likely useful to include a larger window size so that the model has access to more information at fit time. By around day 70 though, this is not entirely true, window that are too large like sizes 30 and 40 produce much weaker AUC scores. The window size of 15, however, remains of high performance, so it would be wise to use window sizes up to 15 during this time of the year. Even maintaining a window size as large as 7 into the beginning and at the end of second quarter seems justified based on continued high performance of the 7 day window during these transitional times. For the third quarter a window of size 15 seems to have the best balance of performance and inclusion of data. For the second and fourth quarters, windows sizes or 1 - 2 days are sufficient for high performance because there is plenty of data amassed even for small windows during this time. By the end of the fourth quarter, however as winter begins to take hold, window sizes of 15 or 20 are best. \newline

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/num_obs.pdf,width=3in,height=4in}}
\caption{AUC score by number of observations for the 2006-2013 time period of observations. Each dot comes from one of the 13 days of the year mentioned above, with one of the 7 window sizes. }
\label{fig6}
\end{figure}

\noindent Another metric that we can use to assess the window size parameter is by the number of points contained within a given window size. Figure 2.3 shows a decreasing trend of AUC values produced by having too large of a window size. Up to around 200 points the models seem to produce high quality results with AUC scores hovering around 0.9. However, once we go past 500 points, AUC scores start to tank, indicating that window sizes generating such high point values are probably too big to provide valuable information.  \newline



\section{Understanding predictor variables}

Before we try to fit models to our observation set, it is necessary to understand what the distributions of our predictor variables look like. This will allow us to identify is there might be some kinds of transformations to the covariates that might be beneficial to our model. In Figure 2.4, we have created histograms of all of the predictor variables for day 150 of 2007. Day 150 is around the height of the first peak in tick activity. We can see that variables like mean air temperature and wind have distributions that appear continuous in nature. However, transpiration rate, wilt and vegetation cover appear to have discrete distributions of values. 

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/2007_day_150.jpg,width=4in,height=4in}}
\caption{Histograms of the distributions of core predictor  variables for day 150 of 2007. }
\label{fig6}
\end{figure}

\noindent The vegetation cover variable is categorical, with 20 different categories, based on the International Geosphere-Biosphere Programme (IGBP) land cover classification system. According to figure 2.4, the most popular vegetation type for tick observations is 11, permanent wetlands, as well as 12-15 which represent cropland and mix vegetation types of forest, shrub and grassland. A more through examination about how vegetation cover of where ticks are found changes throughout the year, reveled another important vegetation type in the second and fourth quarters of the year to be 5, mixed forests. Since a categorical variable with 20 categories does not seem reasonable since so few categories are actually represented, we create a binary indicator variable, called v4 that represents if a tick is found in vegetation type 11-15 or not. 


\section{Poisson point process models for predictor selection and evaluation}

Using the insights that we gained from the exploratory analysis of our dataset we begin with the first step of the ensemble creation process. In section 1.5, we outlined the process for creating an ensemble which involves perturbing initial conditions (in our case window size and study space boundaries), using different model types or creating novel parameter combinations. We will focus on creating novel parameter combinations that are specialized to certain times of the year and using the statistical frame work of the poisson point process model to assess the significance of each of the parameters in the new models. \newline

We build poisson point process models for 13 days of the year (approximately the 15th day of each month is tested); days 15, 46, 76, 105, 135, 146, 166, 196, 227, 258, 288, 319, 349 of the year. We derive the window size parameter by trying to minimize the window while maintaining a good coverage of observation data over the study space. 


\subsection{ Predictor reduction algorithm}

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/summary.png,width=4in,height=4in}}
\caption{Histograms of the distributions of core predictor  variables for day 150 of 2007. }
\label{fig6}
\end{figure}


 \begin{longtable}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \caption{Parameter combinations  and meta data of ensemble members\label{long}}\\

 \hline
 \multicolumn{4}{| c |}{Selected Models}\\
 \hline
 Day of Year & Window Size & Number of Observations & Predictors\\
 \hline
 19   & 20    &12 &   meanAirtemp, vwind, sndepth, sumPrecip \\
 \hline
 19 &   20  & 12   & meanAirtemp, v4 \\
 \hline
 50 & 20 & 11 &  meanAirtemp,  vwind, sncvr\\
 \hline
 50 & 20 & 11 &  meanAirtemp,  vwind, v4\\
 \hline
 78  & 7 & 117 &  meanAirtemp, meanVegcvr, vwind, sumPrecip, sncvr, v4\\
 \hline
 109 &  3   & 118 & meanAirtemp, uwind, meanVegcvr, vwind, minAirtemp, sumPrecip, v4 \\
 \hline
 139 & 3  & 233  & meanAirtemp, uwind, meanVegcvr, vwind, wilt, sumPrecip \\
  \hline
 139 & 3  & 233  & meanAirtemp, meanVegcvr, vwind, sncvr \\
  \hline
 139 & 3  & 233  & meanAirtemp, uwind, meanVegcvr, vwind, wilt, sumPrecip, v4 \\
 \hline
 139 & 3  & 233  & meanAirtemp, meanVegcvr, vwind, sndepth, v4 \\
  \hline
 139 & 3  & 233  & meanAirtemp, meanVegcvr, vwind, sndepth \\
 \hline
 150 & 7  & 445 & meanAirtemp, meanHumidity, meanVegcvr, uwind, vwind, sumPrecip\\
 \hline

 \endfirsthead
 \hline
 \multicolumn{4}{| c |}{Selected Models Continued }\\
 \hline
  Day of Year & Window Size & Number of Observations & Predictors \\
  \hline
  \endhead


 \hline
 150 & 7  & 445 & minAirtemp, meanVegcvr, uwind, vwind, sumPrecip, v4\\
 \hline
150 & 7  & 445 & meanAirtemp, meanHumidity, uwind, sumPrecip, v4\\
 \hline
 150 & 7  & 445 & minAirtemp, meanVegcvr, uwind, vwind, sumPrecip, v4\\
  \hline
 170 & 7  & 262 & meanAirtemp, meanHumidity, uwind, vwind, sumPrecip, v4\\
 \hline
 170 & 7  & 262 & minAirtemp, meanHumidity, uwind, sumPrecip, v4\\
 \hline
 170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, vwind, sumPrecip, v4\\
 \hline
  170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, trnstr, sumPrecip, v4\\
 \hline
   170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, trnstr, sumPrecip, v4\\
 \hline
   170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, vwind, wilt, sumPrecip, v4\\
 \hline
    200 & 10  & 61 & wilt, sumPrecip, v4\\
 \hline
     231 & 13  & 13 & meanAirtemp\\
 \hline
      231 & 13  & 13 & trnstr\\
 \hline
      231 & 13  & 13 & wilt\\
 \hline
       262 & 10  & 62 & meanAirtemp, uwind, vwind, sumPrecip\\
 \hline
        292 & 3  & 553 & meanAirtemp, uwind, meanVegcvr, v4\\
 \hline
         292 & 3  & 553 & maxAirtemp, uwind, wilt, v4\\
 \hline
          292 & 3  & 553 & maxAirtemp, uwind, trnstr, v4)\\
 \hline
          323 & 3  & 250 & meanAirtemp, trnstr, uwind, vwind, v4)\\
 \hline
           323 & 3  & 250 & meanAirtemp, wilt, uwind, vwind, v4)\\
 \hline
            323 & 3  & 250 & meanAirtemp, meanVegcvr, uwind, vwind, v4)\\
 \hline
             353 & 10  & 34 & meanHumidity, uwind, meanVegcvr, v4)\\
 \hline
\end{longtable}

\section{Creating MaxEnt models}

\section{Evaluating MaxEnt models}
