% Chapter 2

\chapter{Exploratory Analysis} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
%\newcommand{\keyword}[1]{\textbf{#1}}
%\newcommand{\tabhead}[1]{\textbf{#1}}
%\newcommand{\code}[1]{\texttt{#1}}
%\newcommand{\file}[1]{\texttt{\bfseries#1}}
%\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Looking at distributions }

We start by looking at how the raw data of our observations set is distributed. We are looking to create a testing and training dataset that are distributed similarly. Figure 2.1 shows two plots of the number of observations recorded on a given day of the year for the time period 2006-2010 (A) and 2011-2013 (B). Although in (A), we see that there are overall more points per day, the yearly shape of observation records is strikingly similar between these two sets. The first and third quarter of the year have few observations per day, while the second and forth quarters of the year have increasing activity, which spikes mid quarter and then declines for the second half of the quarter. 

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/obs_dist.pdf,width=4in,height=3in}}
\caption{Distribution of tick observations during the year for (A) years 2006-2010 and (B) year 2011-2013. }
\label{fig6}
\end{figure}


\noindent Given the nature of this yearly activity cycle of the tick, we need to be cognizant of the number of points being used to create each model. Clearly, there will be a much smaller pool of observations during the first and third quarters, however, many days may have zero observations even as we are aggregating data from a span of almost 10 years. On days with so few data points it becomes impossible to create a model. One way around this obstacle of low observations counts is to pull in observations from a window of time around the forecast date. We will call this new parameter, the window size of the model. \newline

\section{Window size implications }

\noindent The window size parameter has a lot of uncertainty around it. It is unclear what values this parameter should take on at different times of the year. Further we do not know the impact that increasing the size of the window parameter has on the accuracy and precision of the model. One hypothesis is that it is necessary to keep the window size parameter large enough so that the model has exposure to enough data to create well informed predictions. Another hypothesis is that if the window size is increased by too much, then the accuracy of the predictions will be weakened due to the presence of data irrelevant to the current stage of tick activity. \newline

\noindent In order to assess the impact of window size on forecast skill, we run a MaxEnt model with a subset of the 13 core parameters as predictors against 7 candidate window sizes: $\pm$ 2, 3, 7, 15, 20, 30, 40 days, for 13 days of the year (approximately the 15th day of each month is tested); days 15, 46, 76, 105, 135, 146, 166, 196, 227, 258, 288, 319, 349 of the year. \newline

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/wsize.png,width=4in,height=3in}}
\caption{AUC score by day of year for the 2006-2013 time period of observations. Each line represents a different window size from the 7 candidate sizes. }
\label{fig6}
\end{figure}

\noindent The general trend of figure 2.2 independent of window size is the inverse of figure 2.1. Highest AUC's are seen during the first and third quarters of the year, while steep crashes in AUC scores are experienced during the second and forth quarters. It is important to note that as a rule, the fewer points we have in our model, the higher the AUC will be because there is a lower chance of being wrong. For example, the null model of simply guessing that no location has high probability of tick encounter means that with fewer points this model will be more correct than the exact same model judged with more points. However, it is clear that the null model is a very poor choice of a model if future forecast skill is of concern. \newline

\noindent Thus in order to correctly interpret figure 2.2, we focus on the first and third quarters of the year.   



\section{Understanding predictor variables}

\section{Temporal Stationarity Experiments }

The initial experiment in exploratory analysis was to determine how each of the environmental parameters that we are potentially going to be using in the MaxEnt models are related to one another. In order to do this we carry out a correlation analysis to understand how each pairwise set of parameters varies with one another. \newline

\noindent We broke the experiment into four seasons to see if there might be different highly correlated covariates depending on the season. Then we looked at the entire year a more general picture. Below is a section of code used to run the experiment. The dataTools library contains all of the main functions used throughout the code snippet. Here we define 

\begin{figure} [!ht]
\centerline{\psfig{figure=Figures/meta,width=7in}}
\caption{Correlation Analysis between parameters for four seasons, fall (A), summer (B), winter (C), Spring (D) and all year (E).}
\label{fig6}
\end{figure}


\section{Poisson Point Process Models for Predictor Selection and Evaluation}

\subsection{ Building and Interpreting Point Process Models}

\noindent My first look at the Poisson Point Process Models for explaining the distribution of points across the state focused on Day 150 over the years of our experiment 2006-2013. I tried two different window sizes: +/- 3 days and +/- 7 days. Both window periods have lead me to similar conclusions, however the 7 day window had twice as many points so I will discuss my findings for this window.\newline

\noindent Figure 2.2 shows the points observations for the tick sightings on this day of the year for the experiment time period plotted over the state of Maine. We see that there is a good geographic distribution along the highly populated areas of the state. \newline

\noindent We have found three different models that show all highly significant predictors for this day in time:

\begin{equation}
\lambda(i) = e^{-432.54 + 1.37*airtemp^*(i) + 0.49*meanHumidity(i) + 0.038*meanVegcvr(i) - 5.10*uwind(i) + 2.69*vwind(i) + 1.0*sumPrecip(i) }
\end{equation}

\noindent We say $airtemp^* $because you can include here mean, max or min airtemp and come to similar results.


\begin{equation}
\lambda(i) = e^{-483.93 + 1.54*meanAirtemp(i) + 0.58*meanHumidity(i) + 4.46*trnstr(i) - 5*uwind(i) + 1.35*vwind(i) + 0.81*sumPrecip(i) }
\end{equation}
\newline


\section{Predictor Selection Process}

 \begin{longtable}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \caption{Long table caption.\label{long}}\\

 \hline
 \multicolumn{4}{| c |}{Selected Models}\\
 \hline
 Day of Year & Window Size & Number of Observations & Predictors\\
 \hline
 19   & 20    &12 &   meanAirtemp, vwind, sndepth, sumPrecip \\
 \hline
 19 &   20  & 12   & meanAirtemp, v4 \\
 \hline
 50 & 20 & 11 &  meanAirtemp,  vwind, sncvr\\
 \hline
 50 & 20 & 11 &  meanAirtemp,  vwind, v4\\
 \hline
 78  & 7 & 117 &  meanAirtemp, meanVegcvr, vwind, sumPrecip, sncvr, v4\\
 \hline
 109 &  3   & 118 & meanAirtemp, uwind, meanVegcvr, vwind, minAirtemp, sumPrecip, v4 \\
 \hline
 139 & 3  & 233  & meanAirtemp, uwind, meanVegcvr, vwind, wilt, sumPrecip \\
  \hline
 139 & 3  & 233  & meanAirtemp, meanVegcvr, vwind, sncvr \\
  \hline
 139 & 3  & 233  & meanAirtemp, uwind, meanVegcvr, vwind, wilt, sumPrecip, v4 \\
 \hline
 139 & 3  & 233  & meanAirtemp, meanVegcvr, vwind, sndepth, v4 \\
  \hline
 139 & 3  & 233  & meanAirtemp, meanVegcvr, vwind, sndepth \\
 \hline
 150 & 7  & 445 & meanAirtemp, meanHumidity, meanVegcvr, uwind, vwind, sumPrecip\\
 \hline

 \endfirsthead
 \hline
 \multicolumn{4}{| c |}{Selected Models Continued }\\
 \hline
  Day of Year & Window Size & Number of Observations & Predictors \\
  \hline
  \endhead


 \hline
 150 & 7  & 445 & minAirtemp, meanVegcvr, uwind, vwind, sumPrecip, v4\\
 \hline
150 & 7  & 445 & meanAirtemp, meanHumidity, uwind, sumPrecip, v4\\
 \hline
 150 & 7  & 445 & minAirtemp, meanVegcvr, uwind, vwind, sumPrecip, v4\\
  \hline
 170 & 7  & 262 & meanAirtemp, meanHumidity, uwind, vwind, sumPrecip, v4\\
 \hline
 170 & 7  & 262 & minAirtemp, meanHumidity, uwind, sumPrecip, v4\\
 \hline
 170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, vwind, sumPrecip, v4\\
 \hline
  170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, trnstr, sumPrecip, v4\\
 \hline
   170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, trnstr, sumPrecip, v4\\
 \hline
   170 & 7  & 262 & minAirtemp, maxAirtemp, meanHumidity, uwind, vwind, wilt, sumPrecip, v4\\
 \hline
    200 & 10  & 61 & wilt, sumPrecip, v4\\
 \hline
     231 & 13  & 13 & meanAirtemp\\
 \hline
      231 & 13  & 13 & trnstr\\
 \hline
      231 & 13  & 13 & wilt\\
 \hline
       262 & 10  & 62 & meanAirtemp, uwind, vwind, sumPrecip\\
 \hline
        292 & 3  & 553 & meanAirtemp, uwind, meanVegcvr, v4\\
 \hline
         292 & 3  & 553 & maxAirtemp, uwind, wilt, v4\\
 \hline
          292 & 3  & 553 & maxAirtemp, uwind, trnstr, v4)\\
 \hline
          323 & 3  & 250 & meanAirtemp, trnstr, uwind, vwind, v4)\\
 \hline
           323 & 3  & 250 & meanAirtemp, wilt, uwind, vwind, v4)\\
 \hline
            323 & 3  & 250 & meanAirtemp, meanVegcvr, uwind, vwind, v4)\\
 \hline
             353 & 10  & 34 & meanHumidity, uwind, meanVegcvr, v4)\\
 \hline
\end{longtable}


