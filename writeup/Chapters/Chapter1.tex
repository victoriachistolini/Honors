% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

% where does main INTENT of the PROJECT GO? OR is that for the abstract????

\section{What is Species Distribution Modeling}
According to the Center for Disease Control (CDC) in 2015, 95\% of reported Lyme Disease cases came from only 14 states; of these 14, 12 were on the east coast and all 6 of the New England States were represented. Since 1996 the annual number of confirmed cases of Lyme Disease per year has increased by over 10,000 additional cases in 2016 [1]. Given the growing magnitude of the problem presented by ticks and Lyme Disease infection, there has been a deep interest to understand where Lyme Disease carrying ticks are located and how we can most effectively reduce human contact. \newline

\noindent Beginning in 1995, the Maine Medical Center Research Institute (MMCRI) began a project to create detailed records of the locations of discovered infected ticks. Doctors were encouraged to have their patients bring in any ticks that they found on their bodies, in their homes or on their pets, for free testing to determine if the tick were carrying Lyme Disease [2]. Data about the locations of these tick sightings were recorded as part of the study until 2014. \newline

\noindent Species Distribution Modeling (SDM) is a term used to categorize a whole class of models developed for the purpose of understanding the patterns and relationships of an observed species and its environment [3]. Often, the purpose of these models is to predict the range of a species based on where the species has been recorded during surveillance. Another application of SDM models is to predict wether or not a species could be found in a certain location based on the environmental variables of the location. It is with this latter focus that we will pursue Species Distribution models throughout this work. The focus of our research is to develop models to forecast the likelihood of a human-tick encounter in the State of Maine.  \newline

\subsection{Data used in Species Distribution Models}
% mention more detail about data, like predictors (ie raster images each pixel represents a value), each sighting entry has its lat/lon coordinates 
\noindent The database of tick encounters developed by MMCRI houses an enormous amount of information about the distribution of locations where ticks have been found in the state of Maine, however in its raw form, the data is missing a crucial metric: information about where ticks are not found. \newline

\noindent Classical modeling techniques use a set of predictor variables to classify events under certain conditions as likely or unlikely to happen. A logistic regression model, for example could take predictor variables about patients' heart rates and temperatures to determine the likelihood that the patient has the flu. In order to make a good prediction about the patient, the model needs heart rates and temperatures from patients who are healthy and from those who are sick in order to be able to differentiate sick from healthy metrics. In machine learning terminology, models like logistic regression are part of a class of models called supervised learning algorithms, because in order to discern patterns these models need examples of each category to be classified. \newline

\noindent Unfortunately, our tick dataset only provides information about locations where ticks are present and no information about the locations where ticks are absent, which motivates the need to use a different class of models. Unsupervised learning models are able to classify data that has not been labeled, by which group it belongs to, through using the patterns inherent in the data itself to distinguish and predict classification groups. However, in order to use unsupervised algorithms it is necessary that your data contain many examples from each category that you are trying to predict. Thus since our data contain only presence information and no absence data, we are unable build models for our data using unsupervised algorithm as well. \newline

\noindent Clearly when selecting an appropriate model for your data, it is essential to be certain that the type and quality of your data fits what is necessary and expected by the model building procedure to produce respectable results. If the model makes assumptions about your data, which do not hold, then the interpretability and usability of the model will be greatly impaired. In the literature on Species Distribution Modeling, the maximum entropy model is most commonly used, because the assumptions it makes about the data are well aligned to our purpose of modeling, thus we focus our research efforts on creating high preforming models using the maximum entropy model. \newline

%\subsection{Data Quality and Selection Bias} % is this good to have if so where????
%\noindent We need to address selection bias, spacial-auto correlation, correlation with roads, sampling effort is correlated with population density. \newline

%----------------------------------------------------------------------------------------
\section{The Maximum Entropy Model}

To determine the appropriateness of any modeling approach we first start by summarizing what information we know to begin with. We know a sample of locations (latitude and longitude coordinates), where ticks have been found, called presence points. Assuming the presence points are well collected and representative of the locations and environmental conditions where ticks are likely to be found by humans, then we can use this information to estimate and predict locations with high likelihood of encounter. \newline

\noindent We have no information about unsuitable locations, where encounter risk is low, however, we can use our study region, the state of Maine as a constraint on our distribution. By taking a random sample of points from our study region, we create a representation of the diverse habitat conditions in our region, called background points. While we have a high degree of certainty that the presence points represent spaces where ticks should be found, we have no opinion at all about whether or not ticks should be found in the background locations. The theory of minimizing cross entropy ( which can be proved analogous to maximizing entropy)  says that given the information that we do have we can actually find a unique distribution that is optimized to the information that we know and does not penalize us for what we don't know, by selecting the distribution with minimal cross entropy [10].\newline


\noindent More technically put, what we begin with are two probability density functions, $p(x)$ the distribution of tick from our presence and background points and $q(x)$, the probably density function of the tick encounter likelihood conditional on weather conditions and geographic constraints. The process of finding maximum entropy, then is  accomplished by minimize the cross entropy function:  \newline

\begin{equation}
\int q(x)log(\frac{q(x)}{p(x)}) dx
\end{equation}


\noindent  During the minimization process predictor variables undergo transformations that help to maximize the entropy of the solution; this is how the model is fit [6]. The covariates may be transformed into terms of the type : linear, quadratic, product (representing interaction), hinge, threshold or categorical variables in the final model. The effectiveness of the transformations are assessed through cross validation process, using L1 regulation to avoid overfitting [6].  \newline


\subsection{Maximum Entropy Example}

\noindent When we began the modeling process, we had our dataset of tick observations, which highlighted the set of locations where ticks are know to be able to survive, we also start with another subset of point locations in the state of Maine, called the background points, which represent the landscape that we are trying to model. Perhaps the background points dataset consists of two environmental features, elevation and vegetation type look like this: 

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 x & Elevation & Vegetation Type & p(x) \\ [0.5ex] 
 \hline\hline
 1 & H &  1& 0.1  \\ 
 \hline
  2 & H & 3 & 0.1\\
 \hline
   3 & H & 2 & 0.1\\
 \hline
 4 & M & 2 & 0.1  \\
 \hline
  5 & L & 4 & 0.1 \\
 \hline
   6 & L & 3 & 0.1\\
 \hline
  7 & M & 5 & 0.1 \\
 \hline
   8 & M & 4 & 0.1\\
 \hline
 9 & L & 3 & 0.1 \\
 \hline
   10 & M & 3 & 0.1\\
 \hline
\end{tabular}
\end{center}


\noindent We know nothing more than this about the background locations, so we distribute probability to the subset of locations uniformly. That is if we were to go back to each of these locations then the chances of us getting a tick is equally in each location; so if we have $n$ observations in our set, then the probability of getting a tick at any of the locations would be $\frac{1}{n}$. This is a very naive estimate that each location is equiprobable this is our best first guess at the distribution and is called the prior distribution. By preforming a spacial analysis we may be able to find patterns between the locations of our tick sightings in the observations dataset, for example that 90\% of the locations were below a certain elevation. Based on this insight, which we call a constraint, we can recalculate the probabilities predicted for each location in our background point dataset. \newline

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 x & Elevation & Vegetation Type & p(x) \\ [0.5ex] 
 \hline\hline
 1 & H &  1& 1/30 \\ 
 \hline
  2 & H & 3 &  1/30\\
 \hline
   3 & H & 2 &  1/30\\
 \hline
 4 & M & 2 & 9/70  \\
 \hline
  5 & L & 4 & 9/70  \\
 \hline
   6 & L & 3 & 9/70 \\
 \hline
  7 & M & 5 & 9/70 \\
 \hline
   8 & M & 4 &  9/70 \\
 \hline
 9 & L & 3 & 9/70 \\
 \hline
   10 & M & 3 & 9/70\\
 \hline
\end{tabular}
\end{center}

\noindent Next we define another constraint on the model; we find that 82\% of the tick sightings are happening at vegetation types 4 and 5. We can incorporate this new constraint and reweigh the model. This time there is not as clear cut a way to reweigh the probabilities. We know 18\% of the probability should be represented in the set $S_1$= \{ 1, 2, 3, 4, 6, 9,10\} and 80\% should be represented in the set $S_2$ = \{5,7,8 \} but how should the probabilities be determined for each element in the set? As mentioned previously, based on the principle of maximum entropy, we want to select the distribution that is most uniform the distribution by minimizing the entropy function in equation 1.2. This equation is difficult to solve analytically, thus numerical methods will be used for the optimization procedure. This toy example is based off of work done in [11]. \newline

\noindent In building the maxent model distribution, we have used constraints in order to add information to our model. There are several methods for figuring out how to derive the constraint rules. One of these processes is similar to the construction of a decision tree where at each new additional leaf we want to choose the feature that maximizes the information gain. We use the environmental features to develop the constraints which are the expected values of the features.\newline

\subsection{Current MaxEnt model}

\noindent The algorithm that we will be using to create our maximum entropy model is called MaxEnt and is supported by the R programming language through the dismo package; the main algorithm is written in Java, by S.J. Phillips et al. We have currently developed several different MaxEnt models using environmental covariates that best fit the expert knowledge of the parameters which affect tick survival. The core set of environmental covariates come from North American Mesoscale Forecast System (NAM), in the form of raster images of a certain resolution. Each pixel of the image represents a projected latitude, longitude coordinate and the value stored at the pixel is the value of the covariate at that location. For example if the raster image represented elevation, then the value at each pixel, would be the meters above sea level at that location. \newline

\noindent The core parameter set includes: minimum air temperature, maximum air temperature, mean air temperature, mean percent vegetation cover, mean relative humidity, mean snow cover, mean snow depth, mean transpiration rate, mean u-direction wind speed, mean v-direction wind speed, mean wilt, sum of precipitation. Some models include additional parameters for previous-year's minimum winter temperatures.  

\noindent Once a MaxEnt model is fit using the covariates mentioned above, the output model is of the form:
\begin{equation}
q(x) = \frac{e^{\lambda* f(x)}}{Z}
\end{equation}

\noindent where $\lambda $ is a set of weights on the features and Z is a scaling constant that makes sure the probability distribution $q(x)$ sums to 1 [12]. A new model is fit for each day of the year using all of the data in the database available for the particular day of the year. Since often there is not enough data for a single day of the year, data is taken from a window of time around the model day. The size for a suitable window depends on the time of year, with larger windows being needed during time of the year with fewer tick observations.\newline

\noindent Since we have created several MaxEnt models our goal is preform exploratory analysis and look at potential sources of error and model refinement through the creation of new models with time-of-year specific parameter combinations to boost overall system performance. 


%----------------------------------------------------------------------------------------
\section{Spacial-Temporal Exploratory Analysis} 

Another complicating factor of this dataset is that it contains data concerning spacial locations across time; spacial and temporal dimensions require explicit analysis and dictate the use of distinct statistical techniques. Spacial analysis is the use of specialized statistical techniques that are designed to help identify patterns that derive from the spacial dimensions of the data [4]. \newline

\subsection{Spacial Analysis and Predictor Selection}
In species distribution modeling there are two components to the data set that will be used to make predictions on the location of ticks in the environment. We have described the first part of the dataset, the tick observations, a set of latitude, longitude pairs that uniquely define a point in space where a tick has been sighted by a person, as well as the date of observation. The second part of the dataset are the predictors, which are a set of environmental variables that will be used to define the specie's environmental niche [3]. Environmental data, collected by the weather service can be downloaded for the location and date of the sightings and will be stored in memory as a raster object, which is a grid of pixels used to represent spacial data[4].  \newline

\noindent Selection of appropriate predictors to use for model building can be very difficult because it takes a good deal of understanding about the way in which the species interacts with its environment to understand what types of environmental parameters influence a its ability to live in an environment. In our case, many additional factors influence the ability of a tick to survive and disperse throughout its environment such as availability of host animals, whose distribution may also be unknown. Further, once we have an idea of what parameters are appropriate to use in to model, based on the literature of known tick habitat conditions, we must be careful to select only variables that are not correlated with one another to include in model building, otherwise contribution estimates for the predictors are confounded. \newline

\noindent Based on our insights, before attempting to build a predictive model we must preform an exploratory analysis of the relationships between our predictors and our observations. Creating pairwise regression models of each predictor against one another, help us to identify and quantify correlation between covariates. Once we have identified which predictors are significantly correlated with one another, we must select a subset of parameters which optimally characterize tick behavior while reducing the possibility of correlation. For example, if we hypothesize that the two predictors vegetation cover and elevation are important parameters in characterizing a tick's suitable habitat, but find that the type of vegetation in an ecosystem is almost completely determined by the ecosystem's elevation, we cannot include both parameters in our model, but which should we include? \newline

\noindent A branch of spacial analysis called point pattern analysis can help us understand how the tick observation points are distributed differently across different covariates and may help us to further understand the utility of each parameter. There are several types of point pattern analysis algorithms, however, we will focus on two in our analysis: quadrant density and Poisson Point process. The premise of both of these techniques is the same: to understand how the distribution of the observation points relate to the distribution of the covariate. \newline

\noindent In the quadrant density method as the researcher you must look at the distribution of the covariate and break it into discrete categories, in our example of elevation, we might separate the predictor into a high, medium and low elevation group for example and then compute density of the region, defined as the number of points over region area [4]. Since the categorization of the variable is determined at the discretion of the researcher, very different conclusions can be made based on the categorization strategy, which can lead to unreflective conclusions, however the Poison Point process will allow us to use a less biased approach based on creating a logistic regression model of points based on the predictor:

\begin{equation}
\lambda(i) = e^{\alpha +\beta(i)}
\end{equation}

\noindent This logistic model allows the researcher to understand the density of point observations at any arbitrary input location, and thus understand more deeply the relationship that the point observations have with a particular covariate.\newline

% weird should this be included 
\noindent Also, other examples have included the use of PCA analysis as a preprocessing step to reduce the features used in modeling building to independent highly effective features, with reduced dimensionality [8].   \newline


\subsection{Temporal Analysis}
% do predictor sets change over time

% window experiments
\noindent Another temporal component of analysis concerns the amount of point data that the dataset has at different times of the year. Ideally we will be looking to make a model for each day of the year to use for forecast prediction. However, due to the fact that one is less likely to encounter a tick in winter months, we have fewer observations for this time frame. With fewer observations to build the model, the forecast predictions are less accurate.  Thus, we must preform an experiment to understand the impact of the amount of data used to build the model and the model's subsequent accuracy. We seek to understand if adding more data from the observations of surrounding months to the model when there are few observations will positively impact accuracy. We also seek to determine a threshold about the number of observations necessary to achieve a certain error tolerance. \newline

% stationarity of the data set
Another area of interest is the stationarity of the data. There is evidence that the range of suitable tick habitats continues to expand as the effects of global climate change become more dramatic [5]. At this point in our primary analysis we have not explicitly done any analysis on the tick distribution and time. We expect that there will be some correlation with the tick distribution, its predictors and time. In order to test this correlation, we can build regression models for each predictor at each location across time. We may display the resulting correlation coefficients for the a particular covariate at a particular location on a heat map to get a picture of how for a given predictor it correlated throughout time in space.  \newline


%----------------------------------------------------------------------------------------


\section{Model Assessment }  % FIX include stuff about FAUC 

\subsection{Assessment of Performance }

% draw a confusion matrix here....
Assessment of classification algorithms is an essential part of determining their utility as valid predictive models. Classically testing the strength of a classification algorithm involves building the model with a subset of the dataset and keeping another subset for use in testing the model called the training dataset. Then the model is run with the training data and an assessment is made about how well the model has done. The construction of a confusion matrix helps to communicate how well the model did in classifying the training data, by tabulating the number of correctly classified results as well as false positives and false negatives. 

\begin{equation} \bordermatrix{~ & Observed \hspace{0.2cm} Presence & Observed \hspace{0.2cm} Absence \cr
                  Predicted\hspace{0.2cm}  Presence & a & b \cr
                  Predicted \hspace{0.2cm} Absence & c & d \cr} \end{equation}
                  
\noindent The data in the confusion matrix can then be visualized in a plot called the ROC curve. \newline

\subsection{The AUC Statistic for model assessment }
The ROC curve plots the classifier's sensitivity versus 1-specificity, where sensitivity is easily calculated from the confusion matrix as  $\frac{a}{a+c}$ and 1- specificity is $\frac{b}{b+d}$ [7]. Since the results of the MaxEnt model are a series of probabilities given environmental conditions of encountering a tick at a particular location, then in order to create a confusion matrix, we would need to decide on some arbitrary threshold at which we decide that a probability is high enough to be considered a presence of a tick. \newline

\noindent Since the decision of a threshold is arbitrary then ROC is formed from finding the confusion matrix for each threshold, which will give a new sensitivity and 1-specificity value to plot for each matrix. The summary statistic used to characterize the ROC curve is the area under the ROC curve or AUC, which evaluates the strength of the classifier by the characteristics of the ROC curve. An AUC statistic of 0.5 represents a random classifier, and scores above 0.5 represent a better than random model [7].    \newline

\noindent Given the type of data we are using, presence only data, and the fact that we are generating a forecast, the traditional methods of model assessment fall short. Firstly, since the training dataset would be only a series of presence observations, so there would be 0 observations correctly classified for no encounter, cell d,  which would make it impossible to calculate summary statistics, used to calculated the ROC curve. Given these two complicating factors, we will pursue other methods of assessment which employ modifications on the classical techniques to be functional for a presence only dataset.  \newline

\noindent Since we do not have absence data in our dataset and thus our training dataset is devoid of this essential information, then a more accurate substitute is using is using the proportion of area predicted present instead of 1-specificity [7]. With the simple modification under way we can proceed to interpret the AUC in a similar fashion as our interpretations of the traditionally defined AUC.  \newline


\subsection{The F-AUC statistic for assessment of future predictive power }

\subsection{Limitations of AUC Interpretability}
\noindent Another limiting factor that about AUC that we have previously mentioned is that it represents the classification ability of the model independent of a threshold. The threshold determines the minimum probability that will be considered a presence. In our application this threshold is important because we are creating a forecast and the interpretability of a forecast greatly depends on the ability to discriminate events from non-events, thus the there seems to be an implied necessity to define the threshold as a single number when calculating a summary statistic about the skill of the forecast. The statistic True Skill Statistic (TSS):
\begin{equation}
TSS \hspace{0.2cm} = \hspace{0.2cm} sensitivity \hspace{0.2cm} + \hspace{0.2cm} specificity\hspace{0.2cm}  -\hspace{0.2cm} 1 \hspace{0.2cm} =  \hspace{0.2cm} \frac{ad -bc}{(a+c)(b+d)}
\end{equation}
can be used used to quantify the strength of the classifier at a given threshold [9]. The TSS statistic has been shown to have a good behavior and is well correlated with the AUC statistic [9]. However, in order to calculate the TSS statistic we would need to have access to a complete confusion matrix which we do not have, thus in order to use the TSS, we would need to substitute our proxy metric calculated at a given threshold. \newline

%\noindent A few other metrics to consider are forecast AUC, AIC, and the procedure of cross validation which we will discuss later on ....\newline no??



%----------------------------------------------------------------------------------------

\section{Ensemble Models}

\subsection{Communicating Uncertainty}
There is immanent uncertainly in the models that we build to predict human-tick encounters, and clearly communicating this expected margin of error is important for those who seek to use the models. One source of error for our models comes from the data that we use as predictor variables. Our models inherit the uncertainty of the tools used to measure these environmental variables. There is generally well understood data on the accuracy and expected error for satellite weather data. \newline

\noindent Another source of uncertainty and error however, comes from the point observations of the human-tick encounters. Since there is likely sampling bias in the data collection methods due to human activity not being randomly distributed throughout the state of Maine, some areas of the state or more represented and over-sampled, while other areas, particularly in the Northern regions may be left un-sampled. Thus, if we have weather data accurate to the county level in Maine, then we may report our forecast predictions at this level of aggregation, however there will be unequal uncertainty in each of the counties which must be quantified and reported.

 ...\newline

\subsection{Ensembles with MaxEnt}
* model physics \newline
* initial conditions\newline
* training data \newline
* summarizing ensembles -> a bayesian approach \newline


%----------------------------------------------------------------------------------------
